뉴런 = 시냅스 (연결작용)
(인풋과 각 인풋에 대한 가중치 곱) => 활성함수의 변수로
	=> Output 도출 가능

위 계산을 연산량을 줄이기 위해 행렬 방식으로 dot 연산

## 활성함수?

활성 함수는 모델이 복잡한 비선형 관계를 학습하고 표현할 수 있게 도와줍니다. 다양한 종류의 활성 함수가 있으며, 각각의 특징과 용도를 가지고 있습니다.

### 시그모이드 함수

수식: f(x) = 1 / (1 + e^(-x))

시그모이드 함수는 입력을 0 ~ 1 사이의 값으로 변환합니다. 주로 이진 분류 문제에서 출력층에서 사용될 수 있습니다. 그러나 심층 신경망에서는 그레디언트 소실(Vanishing Gradient) 문제를 야기할 가능성이 있습니다.

### 하이퍼볼릭 탄젠트 함수

수식: f(x) = (e^(x) - e^(-x)) / (e^(x) + e^(-x))

하이퍼볼릭 탄젠트 함수는 시그모이드와 유사하지만, 입력을 -1 ~ 1 사이의 값으로 변환합니다. 이로써 양수와 음수를 모두 다룰 수 있는 장점을 가집니다.

### ReLU 함수

수식: f(x) = max(0, x)

ReLU 함수는 입력이 0보다 크면 입력 값을 그대로 출력하고, 0 이하면 0으로 출력합니다. 이는 음수 값을 모두 0으로 처리하며 비선형성을 유지하면서 계산이 효율적입니다. 그러나 음수 값이 많은 경우 해당 뉴런이 활성화되지 않을 가능성이 있습니다.

### Leaky ReLU 함수

수식: f(x) = x, if x > 0; alpha * x, if x <= 0 (alpha는 작은 양수)

Leaky ReLU 함수는 ReLU의 변형으로, 음수를 0이 아닌 작은 값으로 출력하여 음수의 정보를 보존하려는 시도입니다.

### 소프트맥스 함수

수식: f(x_i) = e^(x_i) / ∑(e^(x_j)) for all j

소프트맥스 함수는 다중 클래스 분류 문제에서 사용되며, 입력 값을 확률 분포로 변환하여 각 클래스에 대한 확률 값을 출력으로 반환합니다.

### Gated Recurrent Unit / Long Short-Term Memory

Gated Recurrent Unit (GRU)와 Long Short-Term Memory (LSTM)은 순환 신경망에서 사용되는 특수한 형태의 활성 함수로, 시퀀스 데이터 처리에 활용됩니다.

## Layer의 깊이?

신경망의 층(layer) 깊이는 결과에 영향을 미칩니다.

- 깊은 신경망은 더 복잡한 함수를 모델링할 수 있는 표현 능력을 가집니다. 따라서 더 다양한 특징과 패턴을 학습할 수 있습니다.
- 처음 층들은 간단한 특징을 감지하고, 이후 층들은 이러한 특징을 조합하여 더 의미 있는 특징을 감지합니다. 이를 통해 모델의 표현력이 향상됩니다.
- 그러나 층이 너무 깊으면 과적합 문제가 발생할 수 있습니다. 이를 해결하기 위해 정규화 기법을 사용할 수 있습니다.
- 깊은 신경망은 최적화가 어려울 수 있습니다. 그레디언트 소실이나 폭주 문제가 발생할 수 있는데, 이를 해결하기 위해 초기화나 정규화 기법을 사용합니다.
- 깊은 신경망은 계산 비용이 증가할 수 있습니다. 따라서 계산 리소스와 시간을 고려해야 합니다.

따라서 적절한 층의 깊이 선택은 신경망 구조와 문제에 따라 결정되어야 합니다. 초기 층에서는 간단한 패턴을 학습하고, 깊어질수록 더 복잡한 특징을 학습하는 방식으로 결과를 최적화할 수 있습니다.