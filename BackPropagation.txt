BackPropagation
	오차 최소화
	경사 하강법 사용
	
적절한 가중치를 찾는 과정
	ForwardPass(순전파)
		인풋을 통해 신경망 각 계층에서 출력 값 계산 (활성함수, 가중치 적용)
	Loss Calculation(손실 계산)
		순전파의 결과와 실제 값을 비교해 오차 계산 => 손실 함수를 통해 측정
	Backward Pass(역전파)
		손실 함수의 기울기를 계산하여 각 계층의 가중치와 편향에 대한 그레디언트 계산
		연쇄법칙(Chain Rule)사용
		손실함수에서 시작해 역순으로 각 계층을 거슬러 올라가며 그레디언트 계산
	Gradient Descent(경사하강법)
		계산된 그레디언트로 가중치와 편향 업데이트
		학습률 같은 하이퍼파라미터 사용해 크기 결정
	=====================> 위 과정 여러 번 반복 수행해 오차를 최소화


연쇄법칙?
	미분과 관련
	합성함수의 미분을 구하는 데 사용
	다중 변수로 이루어진 함수의 미분을 분해하여 각 변수에 대한 미분 조합 규칙 제공
	=> 복잡한 함수의 미분 계산 간단히 가능
	

손실함수?
	종류
	MSE(평균 제곱 오차)
	Cross-Entropy Loss
	MAE(평균 절대 오차)


하이퍼 파라미터?
	머신 러닝 모델의 구조와 학습 과정을 제어하는 매개변수
	모델 내부에서 학습되지 않고 사용자가 직접 설정해야 함.
	모델의 성능과 학습 속도에 큰 영향을 미침
	
	종류
		학습률 : 얼마만큼의 스텝을 이동할지 결정 => 적절한 범위 설정
		Epochs : 데이터셋을 몇 번 반복 학습할 지 결정 => overfitting 염두
		BatchSize : 한 번에 업데이트에 사용할 데이터 샘플 개수 결정. 작은 배치 크기는 노이즈가 적지만 큰 계산비용
		신경망의 hiddenLayer 수와 뉴런의 수
		DropOut비율
		정규화 파라미터: L1 or L2 정규화 강도 조절
		커널 크기와 스트라이드 : 합성곱(Convolutional Neural Network, CNN)에서 사용하고, 특징을 추출하는 필터의 크기와 스텝 결정
	=======> 주로 Grid Search or Random Search 같은 방법을 통해 수행
